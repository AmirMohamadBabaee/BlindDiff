import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import math
from basicsr.utils.registry import ARCH_REGISTRY
from .arch_util import Upsample, make_layer, pixel_unshuffle
#  from arch_util import Upsample, make_layer, pixel_unshuffle


class PositionalEncoding(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, noise_level):
        count = self.dim // 2
        step = torch.arange(count, dtype=noise_level.dtype,
                            device=noise_level.device) / count
        encoding = noise_level.unsqueeze(
            1) * torch.exp(-math.log(1e4) * step.unsqueeze(0))
        encoding = torch.cat(
            [torch.sin(encoding), torch.cos(encoding)], dim=-1)
        return encoding

class KAB(nn.Module):
    def __init__(self, num_feat, kernel_size):
        super(KAB, self).__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(num_feat, num_feat, 3, 1, 1),
            nn.Mish(),
        )
        self.block2 = nn.Sequential(
            nn.Conv2d(num_feat, num_feat, 3, 1, 1),
            nn.Mish(),
        )
        self.block3 = nn.Sequential(
            nn.Conv2d(num_feat, num_feat, 3, 1, 1),
            nn.Mish(),
        )
        self.k_mlp = nn.Sequential(
            nn.Linear(num_feat, kernel_size ** 2),
            nn.Mish(),
            nn.Linear(kernel_size ** 2, num_feat)
            )
        self.k_modulation = nn.Sequential(
            nn.Mish(),
            nn.Linear(num_feat, 3 * num_feat, bias=True)
        )
        self.t_modulation = nn.Sequential(
            nn.Mish(),
            nn.Linear(num_feat, 3 * num_feat, bias=True)
        )
        self.conv_shift = nn.Conv2d(num_feat, num_feat, 1, 1, 0)
        self.conv_gate = nn.Conv2d(num_feat, num_feat, 1, 1, 0)
        self.conv_scale = nn.Conv2d(num_feat, num_feat, 1, 1, 0)

    def forward(self, x, t, cond, k):
        scale_t, gate_t, shift_t = self.t_modulation(t).chunk(3,dim=-1)
        scale_t = scale_t.unsqueeze(-1).unsqueeze(-1)
        gate_t = gate_t.unsqueeze(-1).unsqueeze(-1)
        shift_t = shift_t.unsqueeze(-1).unsqueeze(-1)
        k = self.k_mlp(k)
        scale_k, gate_k, shift_k = self.k_modulation(k).chunk(3,dim=-1)
        scale_k = scale_k.unsqueeze(-1).unsqueeze(-1)
        gate_k = gate_k.unsqueeze(-1).unsqueeze(-1)
        shift_k = shift_k.unsqueeze(-1).unsqueeze(-1)
        shift_lr = self.conv_shift(cond)
        scale_lr = self.conv_scale(cond)
        gate_lr = self.conv_gate(cond)
        x = x + gate_t * self.block1(x * (1+scale_t) + shift_t)
        x = x + gate_k * self.block2(x * (1+scale_k) + shift_k)
        x = x + gate_lr * self.block3(x * (1+scale_lr) + shift_lr)
        return x, k

@ARCH_REGISTRY.register()
class KAN(nn.Module):
    def __init__(self, num_in_ch=3,
                 num_out_ch=3,
                 num_feat=128,
                 depth=40,
                 scale=4,
                 kernel_size=21):
        super(KAN, self).__init__()
        
        self.cond_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)
        
        self.predictor=nn.Sequential(
            nn.Conv2d(num_feat, num_feat, 3, 1, 1),
            nn.LeakyReLU(0.2, True),
            nn.Conv2d(num_feat, num_feat, 3, 1, 1),
            nn.AdaptiveAvgPool2d(1),
        )
        
        
        if scale == 2:
            num_in_ch = num_in_ch * 4
        elif scale == 4:
            num_in_ch = num_in_ch * 16
            
        self.conv_first = nn.Conv2d(num_in_ch, num_feat, 3, 1, 1)
        
        self.blocks = nn.ModuleList([
            KAB(num_feat, kernel_size) for _ in range(depth)
        ])
        
        self.t_embedder = nn.Sequential(
                PositionalEncoding(num_feat),
                nn.Linear(num_feat, kernel_size ** 2),
                nn.Mish(),
                nn.Linear(kernel_size ** 2, num_feat)
            )
        self.conv_last = nn.Conv2d(num_feat, 2 * num_out_ch, 3, 1, 1)
        
        self.upsample = Upsample(scale, num_feat)
        
        self.k_mlp = nn.Sequential(
            nn.Linear(num_feat, kernel_size ** 2),
            nn.Mish(),
            nn.Linear(kernel_size ** 2, kernel_size ** 2)
            )

    def forward(self, x, t, cond):
        x = pixel_unshuffle(x, scale=4)
        cond = self.cond_first(cond)
        t_embed = self.t_embedder(t)
        x = self.conv_first(x)
        k = self.predictor(cond).squeeze().squeeze()
        for block in self.blocks:
            x, k = block(x, t_embed, cond, k)
        x = self.upsample(x)
        x = self.conv_last(x)
        k = self.k_mlp(k)

        return x, k

if __name__=='__main__':
    model=KAN().cuda()
    print('# generator parameters:', sum(param.numel() for param in model.parameters()))
    x = torch.randn(32, 3, 128, 128).cuda()
    cond = torch.randn(32, 3, 32, 32).cuda()
    t = torch.randn(32).cuda()
    y,k=model(x,t,cond)
    print(y.shape)
    print(k.shape)